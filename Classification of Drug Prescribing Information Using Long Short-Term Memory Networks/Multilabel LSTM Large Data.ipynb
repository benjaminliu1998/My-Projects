{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrNuITQ5V1Ey"
   },
   "source": [
    "## Data Loading (Saved, Don't have to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFWJha2HWIXA"
   },
   "source": [
    "updates\n",
    "- punctuation not include %\n",
    "- split based on r' |\\[|\\]|\\(|\\)|,|!|\\\"|#|\\$|\\*|:|;|\\?|'\n",
    "- don't use is.alpha / is.numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzMzZfseWAAX"
   },
   "outputs": [],
   "source": [
    "# LOADING TRAINING DATA | LARGE\n",
    "import statistics\n",
    "import numbers\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "stopwords = nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# punctuation manipulation: do not include \"%\"\n",
    "punctuation_updated = '!\"#$&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "image_path = \"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Pediatrics Figures/Multilabel Output Large New/\"\n",
    "train_path = \"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Benjamin Liu/Pediatrics/output/train/\"\n",
    "categories = ['pedia', 'lactation', 'nursing', 'pregnancy','geriatric']\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for index, cat in enumerate(categories):\n",
    "  # ------- for every cat ------- \n",
    "  X_train_cat = []\n",
    "  print(\"Length of \" + cat + \" is: \" + str(len(os.listdir(train_path + cat))))\n",
    "  for file_idx, filename in enumerate(os.listdir(train_path + cat)):\n",
    "    # FOR ONE PARAGRAPH\n",
    "    if (file_idx % 100 == 0):\n",
    "      print(\"Pre-processing \" + cat + \" \" + str(file_idx))\n",
    "    fp = open(train_path + cat + \"/\" + filename)\n",
    "    s = fp.read()\n",
    "    fp.close()\n",
    "    lines = s.split(\"\\n\")\n",
    "    #print(lines)\n",
    "    \n",
    "    # ------- tokenize and data cleaning by cat -----\n",
    "    if cat == \"pedia\":\n",
    "      # tokenize and data cleaning\n",
    "      for i in range(len(lines)):\n",
    "        # remove \"pediatric use\"\n",
    "        if any(elmt in lines[i][:20] for elmt in [\"Pediatric Use\", \"pediatric Use\"]):\n",
    "          lines[i] = lines[i][lines[i].index('Use') + 3 :]\n",
    "        if any(elmt in lines[i][:20] for elmt in [\"Pediatric use\", \"pediatric use\"]):\n",
    "          lines[i] = lines[i][lines[i].index('use') + 3 :]\n",
    "        if lines[i] == \"8.4\":\n",
    "          lines[i] = \" \"\n",
    "        # tokenize \n",
    "        lines[i] = [word.strip(string.punctuation) for word in re.split(' |\\[|\\]|\\(|\\)|,|!|\\\"|#|\\$|\\*|:|;|\\?|', lines[i])]\n",
    "        lines[i] = [word.lower() for word in lines[i]] # normalize\n",
    "      two_d_list = lines\n",
    "    if cat == \"geriatrics\":\n",
    "      # tokenize and data cleaning\n",
    "      for i in range(len(lines)):\n",
    "        # remove \"geriatric use\"\n",
    "        if any(elmt in lines[i][:20] for elmt in [\"Geriatric Use\", \"geriatric Use\"]):\n",
    "          lines[i] = lines[i][lines[i].index('Use') + 3 :]\n",
    "        if any(elmt in lines[i][:20] for elmt in [\"Geriatric use\", \"geriatric use\"]):\n",
    "          lines[i] = lines[i][lines[i].index('use') + 3 :]\n",
    "        if lines[i] == \"8.5\":\n",
    "          lines[i] = \" \"\n",
    "        # tokenize \n",
    "        lines[i] = [word.strip(string.punctuation) for word in re.split(' |\\[|\\]|\\(|\\)|,|!|\\\"|#|\\$|\\*|:|;|\\?|', lines[i])]\n",
    "        lines[i] = [word.lower() for word in lines[i]] # normalize\n",
    "      two_d_list = lines\n",
    "    elif cat == \"lactation\":\n",
    "      # tokenize and data cleaning\n",
    "      for i in range(len(lines)):\n",
    "        # remove \"lactation\"\n",
    "        if any(elmt in lines[i][:20] for elmt in [\"lactation\"]):\n",
    "          lines[i] = lines[i][lines[i].index('lactation') + 9 :]\n",
    "        if any(elmt in lines[i][:20] for elmt in [\"Lactation\"]):\n",
    "          lines[i] = lines[i][lines[i].index('Lactation') + 9 :]\n",
    "        if any(elmt in lines[i][:20] for elmt in [\"Risk Summary\", \"risk Summary\"]):\n",
    "          lines[i] = lines[i][lines[i].index('Summary') + 7 :]\n",
    "        if any(elmt in lines[i][:20] for elmt in [\"Risk summary\", \"risk summary\"]):\n",
    "          lines[i] = lines[i][lines[i].index('summary') + 7 :]\n",
    "        if lines[i] == \"8.2\":\n",
    "          lines[i] = \" \"\n",
    "        # tokenize \n",
    "        lines[i] = [word.strip(string.punctuation) for word in re.split(' |\\[|\\]|\\(|\\)|,|!|\\\"|#|\\$|\\*|:|;|\\?|', lines[i])]\n",
    "        lines[i] = [word.lower() for word in lines[i]] # normalize\n",
    "      two_d_list = lines\n",
    "    elif cat == \"nonpedia\":\n",
    "      # tokenize and data cleaning\n",
    "      for i in range(len(lines)):\n",
    "        # tokenize \n",
    "        lines[i] = [word.strip(string.punctuation) for word in re.split(' |\\[|\\]|\\(|\\)|,|!|\\\"|#|\\$|\\*|:|;|\\?|', lines[i])]\n",
    "        lines[i] = [word.lower() for word in lines[i]] # normalize\n",
    "      two_d_list = lines\n",
    "    elif cat == \"nursing\":\n",
    "      # tokenize and data cleaning\n",
    "      for i in range(len(lines)):\n",
    "        # remove \"pediatric use\"\n",
    "        if any(elmt in lines[i][:21] for elmt in [\"Nursing Mothers\", \"nursing Mothers\"]):\n",
    "          lines[i] = lines[i][lines[i].index('Mother') + 6 :]\n",
    "        if any(elmt in lines[i][:21] for elmt in [\"Nursing mothers\", \"nursing mothers\"]):\n",
    "          lines[i] = lines[i][lines[i].index('mother') + 6 :]\n",
    "        if any(elmt in lines[i][:50] for elmt in [\"Females and Males of Reproductive Potential\"]):\n",
    "          lines[i] = lines[i][lines[i].index('Potential') + 9 :]\n",
    "        if lines[i] == \"8.3\":\n",
    "          lines[i] = \" \"\n",
    "        # tokenize\n",
    "        lines[i] = [word.strip(string.punctuation) for word in re.split(' |\\[|\\]|\\(|\\)|,|!|\\\"|#|\\$|\\*|:|;|\\?|', lines[i])]\n",
    "        lines[i] = [word.lower() for word in lines[i]] # normalize\n",
    "      two_d_list = lines\n",
    "    elif cat == \"pregnancy\":\n",
    "      # tokenize and data cleaning\n",
    "      for i in range(len(lines)):\n",
    "        # remove \"pediatric use\"\n",
    "        if any(elmt in lines[i][:20] for elmt in [\"Pregnancy\"]):\n",
    "          lines[i] = lines[i][lines[i].index('Pregnancy') + 9 :]\n",
    "        if any(elmt in lines[i][:20] for elmt in [\"pregnancy\"]):\n",
    "          lines[i] = lines[i][lines[i].index('pregnancy') + 9 :]\n",
    "        if lines[i] == \"8.1\":\n",
    "          lines[i] = \" \"\n",
    "        # tokenize \n",
    "        lines[i] = [word.strip(string.punctuation) for word in re.split(' |\\[|\\]|\\(|\\)|,|!|\\\"|#|\\$|\\*|:|;|\\?|', lines[i])]\n",
    "        lines[i] = [word.lower() for word in lines[i]] # normalize\n",
    "      two_d_list = lines\n",
    "    # -----------------------------------------\n",
    "\n",
    "    one_parag_tokenized = []\n",
    "    for i in range(len(two_d_list)):\n",
    "      for k, elmt in enumerate(two_d_list[i]): # Traverse thru all\n",
    "        if re.match('[\\s\\S]+\\.+[a-zA-Z]', elmt): # if its something.something\n",
    "          splitted_word = re.split('\\.', elmt)\n",
    "          two_d_list[i].remove(elmt)\n",
    "          for j, wd in enumerate(splitted_word):\n",
    "            two_d_list[i].insert(k+j, wd)\n",
    "        \n",
    "    for i in range(len(two_d_list)):\n",
    "      for k in range(len(two_d_list[i])): # Traverse thru all    \n",
    "        if two_d_list[i][k] != \"\" and two_d_list[i][k] != \" \": # remove empty\n",
    "          one_parag_tokenized.append(two_d_list[i][k])\n",
    "    #print(one_parag_tokenized)\n",
    "    X_train_cat.append(one_parag_tokenized)\n",
    "  \n",
    "  # remove stop words (ADDED)\n",
    "  for i in range(len(X_train_cat)):\n",
    "    X_train_cat[i] = [w for w in X_train_cat[i] if not w in stop_words]\n",
    "\n",
    "  print(X_train_cat)\n",
    "  print(\"Length of \" + cat + \" is: \" + str(len(X_train_cat)))\n",
    "\n",
    "  length_list = []\n",
    "  for i in range(len(X_train_cat)):\n",
    "      length_list.append(len(X_train_cat[i]))\n",
    "  print(cat + \" mean length \" + str(statistics.mean(length_list)))\n",
    "  print(cat + \" stdev length \" + str(statistics.stdev(length_list)))\n",
    "\n",
    "  with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_X_train_Large_'+cat+'','wb') as f:\n",
    "    np.save(f, np.array(X_train_cat), allow_pickle=True)\n",
    "  # with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_X_train_Large_'+cat+'.npy', 'rb') as f:\n",
    "  #   X_train_cat = np.load(f,allow_pickle=True)\n",
    "\n",
    "\n",
    "  # LENGTH DISTRIBUTION\n",
    "  length_list = []\n",
    "  for i in range(len(X_train_cat)):\n",
    "      length_list.append(len(X_train_cat[i]))\n",
    "  print(length_list)\n",
    "\n",
    "  # print('Maximum length: ' + str(max(length_list)))\n",
    "  # print('Maximum length: ' + str(min(length_list)))\n",
    "  # print('Median length: ' + str(statistics.median(length_list)))\n",
    "\n",
    "  import numpy as np\n",
    "  import matplotlib.mlab as mlab\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  num_bins = 100\n",
    "  n, bins, patches = plt.hist(length_list, num_bins, facecolor='blue', alpha=0.5)\n",
    "  plt.xlabel('Length of Text (words)')\n",
    "  plt.ylabel('Count')\n",
    "  plt.savefig(image_path + cat + 'Len Dist Train Large.png')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "  #X_train = X_train + X_train_cat\n",
    "  #print(X_train)\n",
    "\n",
    "  # for i in range(len(X_train_cat)):\n",
    "  #   y_train.append(index) # use \"index\" corresponding to \"cat\" as class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEGpp1OkWkZ7"
   },
   "source": [
    "## Create X_train, y_train, Encoding\n",
    "\n",
    "run with the above section or run just this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5589736,
     "status": "ok",
     "timestamp": 1593754540186,
     "user": {
      "displayName": "Tianen Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB2dTVKtTYQaJ7Mde6czm1OtkBIrThRIiHcng-N6E=s64",
      "userId": "17221004817549659243"
     },
     "user_tz": 240
    },
    "id": "940qUOz1-3_f",
    "outputId": "101d0f59-b657-4245-a5c0-5b7c32081896"
   },
   "outputs": [],
   "source": [
    "\n",
    "import statistics\n",
    "import numbers\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "# image_path = \"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Pediatrics Figures/Multilabel Output Large/\"\n",
    "image_path = \"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Pediatrics Figures/Multilabel Output Large New/\"\n",
    "# data_path = \"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Benjamin Liu/Pediatrics/output/train/\"\n",
    "categories = ['pedia', 'lactation', 'nursing', 'pregnancy','geriatrics'] # order cannot change!\n",
    "\n",
    "# # declared in next section\n",
    "# max_word_length_all = 90\n",
    "# embedding_all = 64\n",
    "# batch_all = 512\n",
    "\n",
    "# with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_X_train_Large_token.npy', 'rb') as f:\n",
    "#   X_train = np.load(f,allow_pickle=True)\n",
    "\n",
    "X_train = []\n",
    "for index, cat in enumerate(categories):\n",
    "  with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_X_train_Large_' + cat + '.npy', 'rb') as f:\n",
    "    X_train_cat = np.load(f,allow_pickle=True)\n",
    "    #print(X_train_cat)\n",
    "  X_train = X_train + list(X_train_cat)\n",
    "\n",
    "with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_y_train_Large_orig.npy', 'rb') as f: # loaded from old folder\n",
    "\n",
    "  y_train = np.load(f,allow_pickle=True)\n",
    "\n",
    "\n",
    "# ------ processing y_train\n",
    "import numpy as np\n",
    "y_train = np.array(y_train)\n",
    "print(y_train)\n",
    "\n",
    "if (len(X_train) == len(y_train)):\n",
    "    print(\"X_train and y_train have the same length\")\n",
    "else:\n",
    "    print(\"X_train and y_train DO NOT have the same length\")\n",
    "\n",
    "from keras.utils.np_utils import to_categorical  # ADDED from kaggle\n",
    "y_train = to_categorical(y_train, num_classes = 5)  # ADDED\n",
    "\n",
    "with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_y_train_Large.npy','wb') as f:\n",
    "    np.save(f, np.array(y_train))\n",
    "# with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_y_train_Large.npy', 'rb') as f:\n",
    "#   y_train = np.load(f,allow_pickle=True)\n",
    "\n",
    "\n",
    "# LENGTH DISTRIBUTION\n",
    "length_list = []\n",
    "for i in range(len(X_train)):\n",
    "    length_list.append(len(X_train[i]))\n",
    "print(length_list)\n",
    "\n",
    "print('Maximum length: ' + str(max(length_list)))\n",
    "print('Minimum length: ' + str(min(length_list)))\n",
    "print('Median length: ' + str(statistics.median(length_list)))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_bins = 100\n",
    "n, bins, patches = plt.hist(length_list, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Length of Text (words)')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(image_path + ' Len Dist Large.png')\n",
    "plt.show()\n",
    "\n",
    "# ---- INTEGER ENCODING ----\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# version 2: each paragraph is a string (FOR ONE HOT ENCODING)\n",
    "X_train_in_string = []\n",
    "for paragraph in X_train:\n",
    "  string_for_paragraph = \"\"\n",
    "  for token in paragraph:\n",
    "    string_for_paragraph = string_for_paragraph + str(token) + \" \"\n",
    "  X_train_in_string.append(string_for_paragraph)\n",
    "print(X_train_in_string[0])\n",
    "\n",
    "corpus = X_train_in_string\n",
    "vectorizer = CountVectorizer()\n",
    "X_encoded = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "vocabulary_dict = { i : vocabulary[i] for i in range(0, len(vocabulary) ) }\n",
    "vocabulary_size = len(vocabulary_dict)\n",
    "print(\"vocab size: \" + str(vocabulary_size))\n",
    "\n",
    "# save vocabulary and vocab size\n",
    "with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_Vocab_Large.npy', 'wb') as f:\n",
    "  np.save(f, np.array(vocabulary),allow_pickle=True)\n",
    "# with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_Vocab_Large.npy', 'rb') as f:\n",
    "#   vocabulary = np.load(f,allow_pickle=True)\n",
    "with open(\"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_Vocabsize_Large\", 'w') as f: \n",
    "  f.write(str(vocabulary_size))\n",
    "# vocabulary_size = int(open(\"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_Vocabsize_Large\").read())\n",
    "# print(\"vocab size is:\" + str(vocabulary_size))\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "  for j in range(len(X_train[i])):\n",
    "    if X_train[i][j] in vocabulary:\n",
    "      X_train[i][j] = list(vocabulary_dict.values()).index(X_train[i][j])\n",
    "  X_train[i] = [p for p in X_train[i] if isinstance(p, numbers.Number)]\n",
    "print(X_train[0])\n",
    "\n",
    "with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_X_train_Large_idx.npy', 'wb') as f:\n",
    "  np.save(f, np.array(X_train),allow_pickle=True)\n",
    "# with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_X_train_Large_idx.npy', 'rb') as f:\n",
    "#   X_train = np.load(f,allow_pickle=True)\n",
    "# ---- INTEGER ENCODING ENDS ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnfnlPuwXAT6"
   },
   "source": [
    "## CV Large Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4891671,
     "status": "ok",
     "timestamp": 1593762427245,
     "user": {
      "displayName": "Tianen Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB2dTVKtTYQaJ7Mde6czm1OtkBIrThRIiHcng-N6E=s64",
      "userId": "17221004817549659243"
     },
     "user_tz": 240
    },
    "id": "DHmLE9weECwZ",
    "outputId": "56d79829-0f14-4d56-de1e-db9a33e3624e"
   },
   "outputs": [],
   "source": [
    "  # ----- CROSS VALIDATION | LARGE | USING SAVED DATA -----\n",
    "  import numpy as np\n",
    "  import matplotlib.mlab as mlab\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  max_word_length_all = 90\n",
    "  embedding_all = 64\n",
    "  batch_all = 512\n",
    "\n",
    "  image_path = \"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Pediatrics Figures/Multilabel Output Large New/\"\n",
    "\n",
    "  vocabulary_size = int(open(\"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_Vocabsize_Large\").read())\n",
    "  print(\"vocab size is: \" + str(vocabulary_size))\n",
    "\n",
    "\n",
    "  # with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_X_train_Large_token.npy', 'rb') as f:\n",
    "  #   X_train = np.load(f,allow_pickle=True)\n",
    "  # print(X_train[0])\n",
    "\n",
    "  with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_y_train_Large.npy', 'rb') as f:\n",
    "    y_train = np.load(f,allow_pickle=True)\n",
    "\n",
    "  with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_y_train_Large_orig.npy', 'rb') as f:\n",
    "    y_train_orig = np.load(f,allow_pickle=True) # just for model prediction / confusion matrix purpose\n",
    "\n",
    "  with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_X_train_Large_idx.npy', 'rb') as f:\n",
    "    X_train = np.load(f,allow_pickle=True)\n",
    "  print(X_train[0])\n",
    "  \n",
    "\n",
    "  # =============================================================================================\n",
    "  #from keras.datasets import imdb\n",
    "  import statistics\n",
    "  import numbers\n",
    "  from keras.preprocessing.text import one_hot\n",
    "  from keras.preprocessing.text import text_to_word_sequence\n",
    "  import pandas as pd\n",
    "  \n",
    "\n",
    "\n",
    "  from keras import backend as K\n",
    "\n",
    "  def recall_m(y_true, y_pred):\n",
    "      true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "      possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "      recall = true_positives / (possible_positives + K.epsilon())\n",
    "      return recall\n",
    "  def precision_m(y_true, y_pred):\n",
    "      true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "      predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "      precision = true_positives / (predicted_positives + K.epsilon())\n",
    "      return precision\n",
    "  def f1_m(y_true, y_pred):\n",
    "      precision = precision_m(y_true, y_pred)\n",
    "      recall = recall_m(y_true, y_pred)\n",
    "      return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "  # pad sequence\n",
    "  from keras.preprocessing import sequence\n",
    "  max_words = max_word_length_all #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "  X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "  #X_train = np.array(list(x for x in X_train))\n",
    "  data_size = len(X_train)\n",
    "  fold_size = data_size//5\n",
    "\n",
    "  from tensorflow.python.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "  from tensorflow.python.keras import Sequential\n",
    "  from sklearn import metrics\n",
    "\n",
    "\n",
    "  # random\n",
    "  from sklearn.utils import shuffle\n",
    "  X, y, y_orig = shuffle(X_train, y_train, y_train_orig, random_state=2020)\n",
    "  #print(X) # FOR DEBUGGING\n",
    "\n",
    "  loss_array = [0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0,] # 30 slots. 0-4: overall. 5-9: pedia ...\n",
    "  accuracy_array = [0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0,]\n",
    "  f1_array = [0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0,]\n",
    "  precision_array = [0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0,]\n",
    "  recall_array =[0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0,]\n",
    "\n",
    "\n",
    "  for i in range(5):\n",
    "    print('----- Iteration ', i, '-----')\n",
    "    X_test_cv = X[i*fold_size:i*fold_size+fold_size]\n",
    "    y_test_cv = y[i*fold_size:i*fold_size+fold_size]\n",
    "    y_orig_test_cv = y_orig[i*fold_size:i*fold_size+fold_size]\n",
    "\n",
    "    X_train_cv = np.concatenate((X[0:i*fold_size], X[i*fold_size+fold_size:data_size]), axis=0)\n",
    "    y_train_cv = np.concatenate((y[0:i*fold_size], y[i*fold_size+fold_size:data_size]), axis=0)\n",
    "\n",
    "\n",
    "    embedding_size = embedding_all # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "    #model.add(Dense(14, input_dim=7,activation=\"relu\"))\n",
    "    #model.add(LSTM(100, input_shape = (2,10)))\n",
    "    #model.add(LSTM(100))\n",
    "    model.add(LSTM(100, dropout=0.5)) # deleted \"stateful = False\", deleted \"input_shape = (2,100)\"\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(Dense(1, activation='softmax'))\n",
    "    model.add(Dense(5, activation='softmax')) # dense(len(cat))\n",
    "    print(model.summary())\n",
    "\n",
    "    # Train\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy',f1_m, precision_m, recall_m]) # instead of \"binary_crossentropy\"\n",
    "    batch_size = batch_all # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    num_epochs = 250\n",
    "\n",
    "    history = model.fit(X_train_cv, y_train_cv, validation_data=(X_test_cv, y_test_cv), batch_size=batch_size, epochs=num_epochs)\n",
    "\n",
    "    # -------- NEW -----------------\n",
    "    # predictionns\n",
    "    y_test_cv_pred = model.predict_classes(X_test_cv, verbose=0) # predicted y_test_cv, not actual\n",
    "    print(\"predicted y:\")\n",
    "    print(y_test_cv_pred)\n",
    "    # confusion matrix\n",
    "    print(metrics.confusion_matrix(y_orig_test_cv, y_test_cv_pred))\n",
    "    print(\"report for fold \" + str(i))\n",
    "    classification_report = pd.DataFrame(metrics.classification_report(y_orig_test_cv, y_test_cv_pred, digits=4, target_names=['Ped', 'Lac', 'Nur', 'Preg', 'Ger'], output_dict=True)).transpose()\n",
    "    print(classification_report)\n",
    "    \n",
    "    accuracy_array[i] = classification_report['f1-score'][5] # ----- OVERALL -----\n",
    "    f1_array[i] = classification_report['f1-score'][7]\n",
    "    precision_array[i] = classification_report['precision'][7]\n",
    "    recall_array[i] = classification_report['recall'][7]\n",
    "\n",
    "    indices = [idex for idex, y in enumerate(y_test_cv) if y[0] == 1] # ----- PEDIA ----- 0\n",
    "    loss_ped, accu_ped, f1_score_ped, precision_ped, recall_ped = model.evaluate(X_test_cv[indices], y_test_cv[indices], verbose=0)\n",
    "    accuracy_array[5+i] = accu_ped\n",
    "    f1_array[5+i] = classification_report['f1-score'][0]\n",
    "    precision_array[5+i] = classification_report['precision'][0]\n",
    "    recall_array[5+i] = classification_report['recall'][0]\n",
    "\n",
    "    indices = [idex for idex, y in enumerate(y_test_cv) if y[1] == 1] # ----- LACTATION ----- 1\n",
    "    loss_lac, accu_lac, f1_score_lac, precision_lac, recall_lac = model.evaluate(X_test_cv[indices], y_test_cv[indices], verbose=0)\n",
    "    accuracy_array[10+i] = accu_lac\n",
    "    f1_array[10+i] = classification_report['f1-score'][1]\n",
    "    precision_array[10+i] = classification_report['precision'][1]\n",
    "    recall_array[10+i] = classification_report['recall'][1]\n",
    "\n",
    "    indices = [idex for idex, y in enumerate(y_test_cv) if y[2] == 1] # ----- NURSING ----- 2\n",
    "    loss_nur, accu_nur, f1_score_nur, precision_nur, recall_nur = model.evaluate(X_test_cv[indices], y_test_cv[indices], verbose=0)\n",
    "    accuracy_array[15+i] = accu_nur\n",
    "    f1_array[15+i] = classification_report['f1-score'][2]\n",
    "    precision_array[15+i] = classification_report['precision'][2]\n",
    "    recall_array[15+i] = classification_report['recall'][2]\n",
    "\n",
    "    indices = [idex for idex, y in enumerate(y_test_cv) if y[3] == 1] # ----- PREGNANCY ----- 3\n",
    "    loss_preg, accu_preg, f1_score_preg, precision_preg, recall_preg = model.evaluate(X_test_cv[indices], y_test_cv[indices], verbose=0)\n",
    "    accuracy_array[20+i] = accu_preg\n",
    "    f1_array[20+i] = classification_report['f1-score'][3]\n",
    "    precision_array[20+i] = classification_report['precision'][3]\n",
    "    recall_array[20+i] = classification_report['recall'][3]\n",
    "\n",
    "    indices = [idex for idex, y in enumerate(y_test_cv) if y[4] == 1] # ----- GERIATRICS ----- 4\n",
    "    loss_ger, accu_ger, f1_score_ger, precision_ger, recall_ger = model.evaluate(X_test_cv[indices], y_test_cv[indices], verbose=0)\n",
    "    accuracy_array[25+i] = accu_ger\n",
    "    f1_array[25+i] = classification_report['f1-score'][4]\n",
    "    precision_array[25+i] = classification_report['precision'][4]\n",
    "    recall_array[25+i] = classification_report['recall'][4]\n",
    "    \n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    #plt.savefig(image_path +' Fold '+ str(i)+' Large Model Acc by Epochs.png')\n",
    "    plt.show()\n",
    "\n",
    "    # plot train and validation loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    #plt.savefig(image_path + ' Fold '+ str(i)+' Large Model Loss by Epochs.png')\n",
    "    plt.show()\n",
    "\n",
    "  # ----- OVERALL ----- (print all metrics)\n",
    "  print(\"----- OVERALL -----\")\n",
    "  print(\"Accuracy Array: \", accuracy_array[0:5])\n",
    "  print('5 fold CV test accuracy: ', sum(accuracy_array[0:5])/5)\n",
    "  print('5 fold CV test loss: ', sum(loss_array[0:5])/5)\n",
    "  print('5 fold CV test f1: ', sum(f1_array[0:5])/5)\n",
    "  print('5 fold CV test precision: ', sum(precision_array[0:5])/5)\n",
    "  print('5 fold CV test recall: ', sum(recall_array[0:5])/5)\n",
    "  # calculating standard deviation of 5 fold\n",
    "  import statistics\n",
    "  print('Standard Deviation of 5 Fold acc: ', statistics.stdev(accuracy_array[0:5]))\n",
    "  print('Standard Deviation of 5 Fold loss: ', statistics.stdev(loss_array[0:5]))\n",
    "  print('Standard Deviation of 5 Fold f1: ', statistics.stdev(f1_array[0:5]))\n",
    "  print('Standard Deviation of 5 Fold pre: ', statistics.stdev(precision_array[0:5]))\n",
    "  print('Standard Deviation of 5 Fold rec: ', statistics.stdev(recall_array[0:5]))\n",
    "  # ----- PEDIA ----- (print part of all metrics)\n",
    "  print(\"----- PEDIA -----\")\n",
    "  print('5 fold CV test accuracy: ', sum(accuracy_array[5:10])/5)\n",
    "  print('5 fold CV test f1: ', sum(f1_array[5:10])/5)\n",
    "  print('5 fold CV test precision: ', sum(precision_array[5:10])/5)\n",
    "  print('5 fold CV test recall: ', sum(recall_array[5:10])/5)\n",
    "  print('Standard Deviation of 5 Fold acc: ', statistics.stdev(accuracy_array[5:10]))\n",
    "  # ----- LACTATION -----\n",
    "  print(\"----- LACTATION -----\")\n",
    "  print('5 fold CV test accuracy: ', sum(accuracy_array[10:15])/5)\n",
    "  print('5 fold CV test f1: ', sum(f1_array[10:15])/5)\n",
    "  print('5 fold CV test precision: ', sum(precision_array[10:15])/5)\n",
    "  print('5 fold CV test recall: ', sum(recall_array[10:15])/5)\n",
    "  print('Standard Deviation of 5 Fold acc: ', statistics.stdev(accuracy_array[10:15]))\n",
    "  # ----- NURSING -----\n",
    "  print(\"----- NURSING -----\")\n",
    "  print('5 fold CV test accuracy: ', sum(accuracy_array[15:20])/5)\n",
    "  print('5 fold CV test f1: ', sum(f1_array[15:20])/5)\n",
    "  print('5 fold CV test precision: ', sum(precision_array[15:20])/5)\n",
    "  print('5 fold CV test recall: ', sum(recall_array[15:20])/5)\n",
    "  print('Standard Deviation of 5 Fold acc: ', statistics.stdev(accuracy_array[15:20]))\n",
    "  # ----- PREGNANCY -----\n",
    "  print(\"----- PREGNANCY -----\")\n",
    "  print('5 fold CV test accuracy: ', sum(accuracy_array[20:25])/5)\n",
    "  print('5 fold CV test f1: ', sum(f1_array[20:25])/5)\n",
    "  print('5 fold CV test precision: ', sum(precision_array[20:25])/5)\n",
    "  print('5 fold CV test recall: ', sum(recall_array[20:25])/5)\n",
    "  print('Standard Deviation of 5 Fold acc: ', statistics.stdev(accuracy_array[20:25]))\n",
    "  # ----- GERIATRICS -----\n",
    "  print(\"----- GERIATRICS -----\")\n",
    "  print('5 fold CV test accuracy: ', sum(accuracy_array[25:30])/5)\n",
    "  print('5 fold CV test f1: ', sum(f1_array[25:30])/5)\n",
    "  print('5 fold CV test precision: ', sum(precision_array[25:30])/5)\n",
    "  print('5 fold CV test recall: ', sum(recall_array[25:30])/5)\n",
    "  print('Standard Deviation of 5 Fold acc: ', statistics.stdev(accuracy_array[25:30]))\n",
    "\n",
    "  with open(image_path + ' Metrics_Large_CV.txt', 'w') as f: \n",
    "      f.write('----- OVERALL -----\\n')\n",
    "      f.write('5F Acc: ' + str(accuracy_array[0:5]).strip('[]') + '\\n')\n",
    "      f.write('5F Loss: ' + str(loss_array[0:5]).strip('[]') + '\\n')\n",
    "      f.write('5F Pre: ' + str(precision_array[0:5]).strip('[]') + '\\n')\n",
    "      f.write('5F Rec: ' + str(recall_array[0:5]).strip('[]') + '\\n')\n",
    "      f.write('5F F1: ' + str(f1_array[0:5]).strip('[]') + '\\n')\n",
    "      f.write('5 fold CV test accuracy: ' + str(sum(accuracy_array[0:5])/5) + '\\n')\n",
    "      f.write('5 fold CV test loss: ' + str(sum(loss_array[0:5])/5) + '\\n')\n",
    "      f.write('5 fold CV test precision: ' + str(sum(precision_array[0:5])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: ' + str(sum(recall_array[0:5])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: '+ str(sum(recall_array[0:5])/5) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold acc: '+ str(statistics.stdev(accuracy_array[0:5])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold loss: '+ str(statistics.stdev(loss_array[0:5])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold f1: '+ str(statistics.stdev(f1_array[0:5])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold pre: '+ str(statistics.stdev(precision_array[0:5])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold rec: '+ str(statistics.stdev(recall_array[0:5])) + '\\n')\n",
    "\n",
    "      f.write('----- PEDIA -----\\n')\n",
    "      f.write('5F Acc: ' + str(accuracy_array[5:10]).strip('[]') + '\\n')\n",
    "      f.write('5F Loss: ' + str(loss_array[5:10]).strip('[]') + '\\n')\n",
    "      f.write('5F Pre: ' + str(precision_array[5:10]).strip('[]') + '\\n')\n",
    "      f.write('5F Rec: ' + str(recall_array[5:10]).strip('[]') + '\\n')\n",
    "      f.write('5F F1: ' + str(f1_array[5:10]).strip('[]') + '\\n')\n",
    "      f.write('5 fold CV test accuracy: ' + str(sum(accuracy_array[5:10])/5) + '\\n')\n",
    "      f.write('5 fold CV test loss: ' + str(sum(loss_array[5:10])/5) + '\\n')\n",
    "      f.write('5 fold CV test precision: ' + str(sum(precision_array[5:10])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: ' + str(sum(recall_array[5:10])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: '+ str(sum(recall_array[5:10])/5) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold acc: '+ str(statistics.stdev(accuracy_array[5:10])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold loss: '+ str(statistics.stdev(loss_array[5:10])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold f1: '+ str(statistics.stdev(f1_array[5:10])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold pre: '+ str(statistics.stdev(precision_array[5:10])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold rec: '+ str(statistics.stdev(recall_array[5:10])) + '\\n')\n",
    "\n",
    "      f.write('----- LACTATION -----\\n')\n",
    "      f.write('5F Acc: ' + str(accuracy_array[10:15]).strip('[]') + '\\n')\n",
    "      f.write('5F Loss: ' + str(loss_array[10:15]).strip('[]') + '\\n')\n",
    "      f.write('5F Pre: ' + str(precision_array[10:15]).strip('[]') + '\\n')\n",
    "      f.write('5F Rec: ' + str(recall_array[10:15]).strip('[]') + '\\n')\n",
    "      f.write('5F F1: ' + str(f1_array[10:15]).strip('[]') + '\\n')\n",
    "      f.write('5 fold CV test accuracy: ' + str(sum(accuracy_array[10:15])/5) + '\\n')\n",
    "      f.write('5 fold CV test loss: ' + str(sum(loss_array[10:15])/5) + '\\n')\n",
    "      f.write('5 fold CV test precision: ' + str(sum(precision_array[10:15])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: ' + str(sum(recall_array[10:15])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: '+ str(sum(recall_array[10:15])/5) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold acc: '+ str(statistics.stdev(accuracy_array[10:15])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold loss: '+ str(statistics.stdev(loss_array[10:15])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold f1: '+ str(statistics.stdev(f1_array[10:15])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold pre: '+ str(statistics.stdev(precision_array[10:15])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold rec: '+ str(statistics.stdev(recall_array[10:15])) + '\\n')\n",
    "\n",
    "      f.write('----- NURSING -----\\n')\n",
    "      f.write('5F Acc: ' + str(accuracy_array[15:20]).strip('[]') + '\\n')\n",
    "      f.write('5F Loss: ' + str(loss_array[15:20]).strip('[]') + '\\n')\n",
    "      f.write('5F Pre: ' + str(precision_array[15:20]).strip('[]') + '\\n')\n",
    "      f.write('5F Rec: ' + str(recall_array[15:20]).strip('[]') + '\\n')\n",
    "      f.write('5F F1: ' + str(f1_array[15:20]).strip('[]') + '\\n')\n",
    "      f.write('5 fold CV test accuracy: ' + str(sum(accuracy_array[15:20])/5) + '\\n')\n",
    "      f.write('5 fold CV test loss: ' + str(sum(loss_array[15:20])/5) + '\\n')\n",
    "      f.write('5 fold CV test precision: ' + str(sum(precision_array[15:20])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: ' + str(sum(recall_array[15:20])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: '+ str(sum(recall_array[15:20])/5) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold acc: '+ str(statistics.stdev(accuracy_array[15:20])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold loss: '+ str(statistics.stdev(loss_array[15:20])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold f1: '+ str(statistics.stdev(f1_array[15:20])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold pre: '+ str(statistics.stdev(precision_array[15:20])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold rec: '+ str(statistics.stdev(recall_array[15:20])) + '\\n')\n",
    "\n",
    "      f.write('----- PREGNANCY -----\\n')\n",
    "      f.write('5F Acc: ' + str(accuracy_array[20:25]).strip('[]') + '\\n')\n",
    "      f.write('5F Loss: ' + str(loss_array[20:25]).strip('[]') + '\\n')\n",
    "      f.write('5F Pre: ' + str(precision_array[20:25]).strip('[]') + '\\n')\n",
    "      f.write('5F Rec: ' + str(recall_array[20:25]).strip('[]') + '\\n')\n",
    "      f.write('5F F1: ' + str(f1_array[20:25]).strip('[]') + '\\n')\n",
    "      f.write('5 fold CV test accuracy: ' + str(sum(accuracy_array[20:25])/5) + '\\n')\n",
    "      f.write('5 fold CV test loss: ' + str(sum(loss_array[20:25])/5) + '\\n')\n",
    "      f.write('5 fold CV test precision: ' + str(sum(precision_array[20:25])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: ' + str(sum(recall_array[20:25])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: '+ str(sum(recall_array[20:25])/5) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold acc: '+ str(statistics.stdev(accuracy_array[20:25])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold loss: '+ str(statistics.stdev(loss_array[20:25])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold f1: '+ str(statistics.stdev(f1_array[20:25])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold pre: '+ str(statistics.stdev(precision_array[20:25])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold rec: '+ str(statistics.stdev(recall_array[20:25])) + '\\n')\n",
    "\n",
    "      f.write('----- GERIATRICS -----\\n')\n",
    "      f.write('5F Acc: ' + str(accuracy_array[25:30]).strip('[]') + '\\n')\n",
    "      f.write('5F Loss: ' + str(loss_array[25:30]).strip('[]') + '\\n')\n",
    "      f.write('5F Pre: ' + str(precision_array[25:30]).strip('[]') + '\\n')\n",
    "      f.write('5F Rec: ' + str(recall_array[25:30]).strip('[]') + '\\n')\n",
    "      f.write('5F F1: ' + str(f1_array[25:30]).strip('[]') + '\\n')\n",
    "      f.write('5 fold CV test accuracy: ' + str(sum(accuracy_array[25:30])/5) + '\\n')\n",
    "      f.write('5 fold CV test loss: ' + str(sum(loss_array[25:30])/5) + '\\n')\n",
    "      f.write('5 fold CV test precision: ' + str(sum(precision_array[25:30])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: ' + str(sum(recall_array[25:30])/5) + '\\n')\n",
    "      f.write('5 fold CV test recall: '+ str(sum(recall_array[25:30])/5) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold acc: '+ str(statistics.stdev(accuracy_array[25:30])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold loss: '+ str(statistics.stdev(loss_array[25:30])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold f1: '+ str(statistics.stdev(f1_array[25:30])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold pre: '+ str(statistics.stdev(precision_array[25:30])) + '\\n')\n",
    "      f.write('Standard Deviation of 5 Fold rec: '+ str(statistics.stdev(recall_array[25:30])) + '\\n')\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tr78CnBFT8RY"
   },
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2588076,
     "status": "ok",
     "timestamp": 1593768604487,
     "user": {
      "displayName": "Tianen Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB2dTVKtTYQaJ7Mde6czm1OtkBIrThRIiHcng-N6E=s64",
      "userId": "17221004817549659243"
     },
     "user_tz": 240
    },
    "id": "-VrQhbGGW1kM",
    "outputId": "a486d91c-384d-40ad-bae1-25d0a0d599f3"
   },
   "outputs": [],
   "source": [
    "# Test on combined data\n",
    "\n",
    "# ============== TESTING | TEST 2 | USING SAVED DATA ===========================================\n",
    "# from keras.datasets import imdb\n",
    "import statistics\n",
    "import numbers\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_path = \"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Pediatrics Figures/Multilabel Output Large New/\"\n",
    "\n",
    "\n",
    "max_word_length_all = 90\n",
    "embedding_all = 64\n",
    "batch_all = 512\n",
    "\n",
    "vocabulary_size = int(open(\n",
    "    \"/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_Vocabsize_Large\").read())\n",
    "print(\"vocab size is: \" + str(vocabulary_size))\n",
    "\n",
    "# ---------------------------------------- Load X_train / y_train ----------------------\n",
    "# with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_X_train_Large_token.npy', 'rb') as f:\n",
    "#   X_train = np.load(f,allow_pickle=True)\n",
    "# print(X_train[0])\n",
    "\n",
    "with open(\n",
    "        '/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_y_train_Large.npy',\n",
    "        'rb') as f:\n",
    "    y_train = np.load(f, allow_pickle=True)\n",
    "print(y_train[0])\n",
    "\n",
    "with open(\n",
    "        '/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_X_train_Large_idx.npy',\n",
    "        'rb') as f:\n",
    "    X_train = np.load(f, allow_pickle=True)\n",
    "print(X_train[0])\n",
    "\n",
    "\n",
    "# ---------------------------------------- Load X_test_2 / y_test_2 ----------------------\n",
    "# with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_X_test_2_token.npy', 'rb') as f:\n",
    "#   X_test_2 = np.load(f,allow_pickle=True)\n",
    "\n",
    "with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_y_test.npy', 'rb') as f:\n",
    "  y_test = np.load(f,allow_pickle=True)\n",
    "print(y_test[0])\n",
    "\n",
    "with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_y_test_orig.npy', 'rb') as f:\n",
    "    y_test_orig = np.load(f,allow_pickle=True) # just for model prediction / confusion matrix purpose\n",
    "\n",
    "with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_X_test_token.npy', 'rb') as f:\n",
    "  X_test = np.load(f,allow_pickle=True)\n",
    "print(X_test[0])\n",
    "\n",
    "# ---- INTEGER ENCODING ----\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data New/Multi_Vocab_Large.npy', 'rb') as f:\n",
    "  vocabulary = np.load(f,allow_pickle=True)\n",
    "vocabulary_dict = { i : vocabulary[i] for i in range(0, len(vocabulary) ) }\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "  for j in range(len(X_test[i])):\n",
    "    if X_test[i][j] in vocabulary:\n",
    "      X_test[i][j] = list(vocabulary_dict.values()).index(X_test[i][j])\n",
    "  X_test[i] = [p for p in X_test[i] if isinstance(p, numbers.Number)]\n",
    "\n",
    "# with open('/content/drive/My Drive/#7 Fall 2019 Senior/CSC Honor Research/Pediatrics/Saved Data/Multi_X_test_idx.npy', 'wb') as f:\n",
    "#   np.save(f, X_test, allow_pickle=True)\n",
    "print(X_test[0])\n",
    "\n",
    "\n",
    "\n",
    "import statistics\n",
    "import numbers\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "# pad sequence\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_words = max_word_length_all  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "from tensorflow.python.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.python.keras import Sequential\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# X_train, y_train = shuffle(X_train, y_train, random_state=2020)\n",
    "# X_train = np.concatenate((X_train[0:0], X_train[0:len(X_train)]), axis=0)\n",
    "# y_train = np.concatenate((y_train[0:0], y_train[0:len(y_train)]), axis=0)\n",
    "#X_train = np.concatenate((X_train, []), axis=0)\n",
    "#y_train = np.concatenate((y_train, [[],[],[],[],[],[]]), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embedding_size = embedding_all  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "# model.add(Dense(14, input_dim=7,activation=\"relu\"))\n",
    "# model.add(LSTM(100, input_shape = (2,10)))\n",
    "# model.add(LSTM(100))\n",
    "model.add(LSTM(100, dropout=0.5))  # deleted \"stateful = False\", deleted \"input_shape = (2,100)\"\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='softmax'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# Train\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', f1_m, precision_m, recall_m])  # instead of \"binary_crossentropy\"\n",
    "batch_size = batch_all  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "num_epochs = 250\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=num_epochs)\n",
    "\n",
    "# -------- NEW -----------------\n",
    "# predictionns\n",
    "y_test_pred = model.predict_classes(X_test, verbose=0)  # predicted y_test, not actual\n",
    "print(\"predicted y:\")\n",
    "print(y_test_pred)\n",
    "# confusion matrix\n",
    "print(metrics.confusion_matrix(y_test_orig, y_test_pred))\n",
    "print(\"report for fold \" + str(i))\n",
    "classification_report = pd.DataFrame(metrics.classification_report(y_test_orig, y_test_pred, digits=4,\n",
    "                                                                   target_names=['Ped', 'Lac', 'Nur', 'Preg', 'Ger'],\n",
    "                                                                   output_dict=True)).transpose()\n",
    "print(classification_report)\n",
    "\n",
    "loss, accu, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=0) # ----- OVERALL -----\n",
    "accu = classification_report['f1-score'][5] \n",
    "f1_score = classification_report['f1-score'][7]\n",
    "precision = classification_report['precision'][7]\n",
    "recall = classification_report['recall'][7]\n",
    "\n",
    "indices = [idex for idex, y in enumerate(y_test) if y[0] == 1]  # ----- PEDIA ----- 0\n",
    "loss_ped, accu_ped, f1_score_ped, precision_ped, recall_ped = model.evaluate(X_test[indices], y_test[indices],\n",
    "                                                                             verbose=0)\n",
    "accu_ped = accu_ped\n",
    "f1_score_ped = classification_report['f1-score'][0]\n",
    "precision_ped = classification_report['precision'][0]\n",
    "recall_ped = classification_report['recall'][0]\n",
    "\n",
    "indices = [idex for idex, y in enumerate(y_test) if y[1] == 1]  # ----- LACTATION ----- 1\n",
    "loss_lac, accu_lac, f1_score_lac, precision_lac, recall_lac = model.evaluate(X_test[indices], y_test[indices],\n",
    "                                                                             verbose=0)\n",
    "accu_lac = accu_lac\n",
    "f1_score_lac = classification_report['f1-score'][1]\n",
    "precision_lac = classification_report['precision'][1]\n",
    "recall_lac = classification_report['recall'][1]\n",
    "\n",
    "indices = [idex for idex, y in enumerate(y_test) if y[2] == 1]  # ----- NURSING ----- 2\n",
    "loss_nur, accu_nur, f1_score_nur, precision_nur, recall_nur = model.evaluate(X_test[indices], y_test[indices],\n",
    "                                                                             verbose=0)\n",
    "accu_nur = accu_nur\n",
    "f1_score_nur = classification_report['f1-score'][2]\n",
    "precision_nur = classification_report['precision'][2]\n",
    "recall_nur = classification_report['recall'][2]\n",
    "\n",
    "indices = [idex for idex, y in enumerate(y_test) if y[3] == 1]  # ----- PREGNANCY ----- 3\n",
    "loss_preg, accu_preg, f1_score_preg, precision_preg, recall_preg = model.evaluate(X_test[indices],\n",
    "                                                                                  y_test[indices], verbose=0)\n",
    "accu_preg = accu_preg\n",
    "f1_score_preg = classification_report['f1-score'][3]\n",
    "precision_preg = classification_report['precision'][3]\n",
    "recall_preg = classification_report['recall'][3]\n",
    "\n",
    "indices = [idex for idex, y in enumerate(y_test) if y[4] == 1]  # ----- GERIATRICS ----- 4\n",
    "loss_ger, accu_ger, f1_score_ger, precision_ger, recall_ger = model.evaluate(X_test[indices], y_test[indices],\n",
    "                                                                             verbose=0)\n",
    "accu_ger = accu_ger\n",
    "f1_score_ger = classification_report['f1-score'][4]\n",
    "precision_ger = classification_report['precision'][4]\n",
    "recall_ger = classification_report['recall'][4]\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left', frameon=False)\n",
    "#plt.savefig(image_path + 'Test Acc by Epochs.png')\n",
    "plt.show()\n",
    "\n",
    "# plot train and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right', frameon=False)\n",
    "plt.savefig(image_path + 'Test Loss by Epochs.png')\n",
    "plt.savefig(image_path + 'Test Loss by Epochs_Large.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ----- OVERALL -----\n",
    "print(\"----- OVERALL -----\")\n",
    "# scores = model.evaluate(X_test_cv, y_test_cv, verbose=0)\n",
    "#loss, accu, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Test Loss:', loss)\n",
    "print('Test accuracy:', accu)\n",
    "print('Test f1:', f1_score)\n",
    "print('Test precision:', precision)\n",
    "print('Test recall:', recall)\n",
    "\n",
    "\n",
    "# ----- PEDIA ----- 0\n",
    "print(\"----- PEDIA -----\")\n",
    "#indices = [idex for idex, y in enumerate(y_test) if y[0] == 1]\n",
    "#loss_ped, accu_ped, f1_score_ped, precision_ped, recall_ped = model.evaluate(X_test[indices], y_test[indices],\n",
    "#                                                                             verbose=0)\n",
    "print('Loss:', loss_ped)\n",
    "print('Test accuracy:', accu_ped)\n",
    "print('Test f1:', f1_score_ped)\n",
    "print('Test precision:', precision_ped)\n",
    "print('Test recall:', recall_ped)\n",
    "\n",
    "# ----- LACTATION ----- 1\n",
    "print(\"----- LACTATION -----\")\n",
    "#indices = [idex for idex, y in enumerate(y_test) if y[1] == 1]\n",
    "#loss_lac, accu_lac, f1_score_lac, precision_lac, recall_lac = model.evaluate(X_test[indices], y_test[indices],\n",
    "#                                                                             verbose=0)\n",
    "print('Loss:', loss_lac)\n",
    "print('Test accuracy:', accu_lac)\n",
    "print('Test f1:', f1_score_lac)\n",
    "print('Test precision:', precision_lac)\n",
    "print('Test recall:', recall_lac)\n",
    "\n",
    "# ----- NURSING ----- 2\n",
    "print(\"----- NURSING -----\")\n",
    "#indices = [idex for idex, y in enumerate(y_test) if y[2] == 1]\n",
    "#loss_nur, accu_nur, f1_score_nur, precision_nur, recall_nur = model.evaluate(X_test[indices], y_test[indices],\n",
    "#                                                                             verbose=0)\n",
    "print('Loss:', loss_nur)\n",
    "print('Test accuracy:', accu_nur)\n",
    "print('Test f1:', f1_score_nur)\n",
    "print('Test precision:', precision_nur)\n",
    "print('Test recall:', recall_nur)\n",
    "\n",
    "# ----- PREGNANCY ----- 3\n",
    "print(\"----- PREGNANCY -----\")\n",
    "#indices = [idex for idex, y in enumerate(y_test) if y[3] == 1]\n",
    "#loss_preg, accu_preg, f1_score_preg, precision_preg, recall_preg = model.evaluate(X_test[indices],\n",
    "#                                                                                  y_test[indices], verbose=0)\n",
    "print('Loss:', loss_preg)\n",
    "print('Test accuracy:', accu_preg)\n",
    "print('Test f1:', f1_score_preg)\n",
    "print('Test precision:', precision_preg)\n",
    "print('Test recall:', recall_preg)\n",
    "\n",
    "# ----- GERIATRICS ----- 4\n",
    "print(\"----- GERIATRICS -----\")\n",
    "#indices = [idex for idex, y in enumerate(y_test) if y[4] == 1]\n",
    "#loss_ger, accu_ger, f1_score_ger, precision_ger, recall_ger = model.evaluate(X_test[indices], y_test[indices],\n",
    "#                                                                             verbose=0)\n",
    "print('Loss:', loss_ger)\n",
    "print('Test accuracy:', accu_ger)\n",
    "print('Test f1:', f1_score_ger)\n",
    "print('Test precision:', precision_ger)\n",
    "print('Test recall:', recall_ger)\n",
    "\n",
    "\n",
    "with open(image_path + 'Test Metrics_Large.txt', 'w') as f:\n",
    "    f.write(\"----- OVERALL -----\\n\")\n",
    "    f.write('Test Acc: ' + str(accu) + '\\n')\n",
    "    f.write('Test Loss: ' + str(loss) + '\\n')\n",
    "    f.write('Test Pre: ' + str(precision) + '\\n')\n",
    "    f.write('Test Rec: ' + str(recall) + '\\n')\n",
    "    f.write('Test F1: ' + str(f1_score) + '\\n')\n",
    "\n",
    "    f.write(\"----- PEDIA -----\\n\")\n",
    "    f.write('Test Acc: ' + str(accu_ped) + '\\n')\n",
    "    f.write('Test Loss: ' + str(loss_ped) + '\\n')\n",
    "    f.write('Test Pre: ' + str(precision_ped) + '\\n')\n",
    "    f.write('Test Rec: ' + str(recall_ped) + '\\n')\n",
    "    f.write('Test F1: ' + str(f1_score_ped) + '\\n')\n",
    "\n",
    "    f.write(\"----- LACTATION -----\\n\")\n",
    "    f.write('Test Acc: ' + str(accu_lac) + '\\n')\n",
    "    f.write('Test Loss: ' + str(loss_lac) + '\\n')\n",
    "    f.write('Test Pre: ' + str(precision_lac) + '\\n')\n",
    "    f.write('Test Rec: ' + str(recall_lac) + '\\n')\n",
    "    f.write('Test F1: ' + str(f1_score_lac) + '\\n')\n",
    "\n",
    "    f.write(\"----- NURSING -----\\n\")\n",
    "    f.write('Test Acc: ' + str(accu_nur) + '\\n')\n",
    "    f.write('Test Loss: ' + str(loss_nur) + '\\n')\n",
    "    f.write('Test Pre: ' + str(precision_nur) + '\\n')\n",
    "    f.write('Test Rec: ' + str(recall_nur) + '\\n')\n",
    "    f.write('Test F1: ' + str(f1_score_nur) + '\\n')\n",
    "\n",
    "    f.write(\"----- PREGNANCY -----\\n\")\n",
    "    f.write('Test Acc: ' + str(accu_preg) + '\\n')\n",
    "    f.write('Test Loss: ' + str(loss_preg) + '\\n')\n",
    "    f.write('Test Pre: ' + str(precision_preg) + '\\n')\n",
    "    f.write('Test Rec: ' + str(recall_preg) + '\\n')\n",
    "    f.write('Test F1: ' + str(f1_score_preg) + '\\n')\n",
    "\n",
    "    f.write(\"----- GERIATRICS -----\\n\")\n",
    "    f.write('Test Acc: ' + str(accu_ger) + '\\n')\n",
    "    f.write('Test Loss: ' + str(loss_ger) + '\\n')\n",
    "    f.write('Test Pre: ' + str(precision_ger) + '\\n')\n",
    "    f.write('Test Rec: ' + str(recall_ger) + '\\n')\n",
    "    f.write('Test F1: ' + str(f1_score_ger) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1593768948125,
     "user": {
      "displayName": "Tianen Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB2dTVKtTYQaJ7Mde6czm1OtkBIrThRIiHcng-N6E=s64",
      "userId": "17221004817549659243"
     },
     "user_tz": 240
    },
    "id": "9jBlD3g3BNmh",
    "outputId": "f708e7c1-5bb0-4d69-e4f3-25cfc8d5f817"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(-0.1,1.7)\n",
    "plt.legend(['Train', 'Test'], loc='upper right', frameon=False)\n",
    "plt.savefig(image_path + 'Test Loss by Epochs.png')\n",
    "plt.savefig(image_path + 'Test Loss by Epochs_Large2.pdf')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Multilabel LSTM Large Data.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
